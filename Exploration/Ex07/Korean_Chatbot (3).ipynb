{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2QXixVSvesM5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m4WEK9Ae66f"
      },
      "source": [
        "### 1. 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/sample_data/ChatbotData.csv\""
      ],
      "metadata": {
        "id": "b1xZQHohmJ0O"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_path)#데이터 로드\n",
        "data = data[['Q', 'A']]\n",
        "data = data.dropna()\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuCCv6RbmBSr",
        "outputId": "5d51b143-84a0-400b-a44d-16fac86fb77d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q            A\n",
            "0           12시 땡!   하루가 또 가네요.\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()\n",
        "\n",
        "print(\"총 데이터 수 :\", len(data))\n",
        "data_list = data.values.tolist()\n",
        "print(data_list[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIP_3UzrBBvC",
        "outputId": "4a908dbc-e457-4187-b859-0a1e5be19d2a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 데이터 수 : 11823\n",
            "[['12시 땡!', '하루가 또 가네요.'], ['1지망 학교 떨어졌어', '위로해 드립니다.'], ['3박4일 놀러가고 싶다', '여행은 언제나 좋죠.'], ['3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠.'], ['PPL 심하네', '눈살이 찌푸려지죠.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3kDOCxNe9JG"
      },
      "source": [
        "### 2. 데이터 전처리 하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "3S7e0QUvj0Ov"
      },
      "outputs": [],
      "source": [
        "#전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "\n",
        "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
        "  # student와 온점 사이에 거리를 만듭니다.\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,0-9가-힣]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdkdhGa-gwGP",
        "outputId": "6cc2fc85-4499-4b9f-c75e-3b49a5b8128c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 후의 1000번째 질문 샘플: 노래방 걸 거 같은데 뭐 부르지\n",
            "전처리 후의 1000번째 답변 샘플: 달달한 노래요 .\n"
          ]
        }
      ],
      "source": [
        "questions = data[\"Q\"].apply(preprocess_sentence).tolist()\n",
        "answers = data[\"A\"].apply(preprocess_sentence).tolist()\n",
        "\n",
        "print('전처리 후의 1000번째 질문 샘플: {}'.format(questions[1000]))\n",
        "print('전처리 후의 1000번째 답변 샘플: {}'.format(answers[1000]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. SubwordTextEncoder"
      ],
      "metadata": {
        "id": "XXB--NR_CLQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    questions + answers, target_vocab_size=2**13\n",
        ")\n",
        "\n",
        "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "print('vocab size :', [tokenizer.vocab_size])\n",
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
        "\n",
        "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8l1fq31CRbz",
        "outputId": "891e32ce-e96b-4fc6-8505-c68ab4e99a00"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size : [8162]\n",
            "START_TOKEN의 번호 : [8162]\n",
            "END_TOKEN의 번호 : [8163]\n",
            "8164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 각 단어를 고유한 정수로 인코딩 하고 패딩하기"
      ],
      "metadata": {
        "id": "9UXVu1HqDISJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 1000번째 샘플에 대해서 토큰화 작업을 수행.\n",
        "# 각 토큰을 고유한 정수로 변환\n",
        "print('토큰화 후의 1000번째 질문 샘플: {}'.format(tokenizer.encode(questions[1000])))\n",
        "print('토큰화 후의 1000번째 답변 샘플: {}'.format(tokenizer.encode(answers[1000])))\n",
        "\n",
        "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
        "MAX_LENGTH = 20 ## 15 <- 40"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjasjxWTDOui",
        "outputId": "b63c1038-73ff-4c24-a4f9-50139962d233"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "토큰화 후의 1000번째 질문 샘플: [2120, 85, 8, 444, 156, 2301, 47]\n",
            "토큰화 후의 1000번째 답변 샘플: [3629, 1346, 17, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    # 최대 길이 40->15으로 모든 데이터셋을 패딩 -> postd와 pre 중 실험 해볼 것\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post'\n",
        "    )\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post'\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n"
      ],
      "metadata": {
        "id": "j6GTFEvGDlfK"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "4DpiIjcZlUK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8647849-801c-4fa6-e0a3-9978e5844afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어장의 크기 : 8164\n",
            "필터링 후의 질문 샘플 개수: 11792\n",
            "필터링 후의 답변 샘플 개수: 11792\n"
          ]
        }
      ],
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "print('단어장의 크기 :',(VOCAB_SIZE))\n",
        "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
        "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Teacher Forcing 사용하기"
      ],
      "metadata": {
        "id": "iHJUZG6hEAqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "## 데이터가 디스크에서 한 번만 로드되도록 하여 성능을 향상\n",
        "dataset = dataset.cache()\n",
        "\n",
        "## BUFFER_SIZE만큼 데이터를 섞음\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "\n",
        "## BATCH_SIZE만큼의 샘플로 묶음\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "## 모델이 학습을 하는 동안 다음 배치를 미리 준비\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Uoby8IGoEAN7"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Model 구성하기"
      ],
      "metadata": {
        "id": "gsXCX_opENyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding Layer"
      ],
      "metadata": {
        "id": "zR8qCN6iESF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 포지셔널 인코딩 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    ## 각도 배열 생성\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model\n",
        "        )\n",
        "\n",
        "        ## 배열의 짝수 인덱스에는 sin 함수 적용\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        ## 배열의 홀수 인덱스에는 cosine 함수 적용\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        ## sin과 cosine이 교차되도록 재배열\n",
        "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0])\n",
        "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "p4kq2i2uXZRJ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaled Dot Product Attention"
      ],
      "metadata": {
        "id": "hE53_TBzEYgi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "g00CXUlWlaVx"
      },
      "outputs": [],
      "source": [
        "## 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    ## 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    ## 가중치를 정규화\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    ## 패딩에 마스크 추가\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    ## softmax적용\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    ## 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8BbOESvgAH9"
      },
      "source": [
        "Multi Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "D0qkdsfTkVxX"
      },
      "outputs": [],
      "source": [
        "## MultiHeadAttention 구현\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads ## multi heads 수\n",
        "        self.d_model = d_model ## embedding layer 수\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth)\n",
        "            )\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # Q, K, V에 각각 Dense를 적용합니다\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # 스케일드 닷 프로덕트 어텐션 함수\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masking"
      ],
      "metadata": {
        "id": "iOsUolt3Eml7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "iAwNv_5gewcS"
      },
      "outputs": [],
      "source": [
        "## Padding Masking\n",
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "## Look-ahead masking\n",
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "2Vg10b9bEtvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "GNvDMZKYiotG"
      },
      "outputs": [],
      "source": [
        "#인코더 레이어\n",
        "# 인코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "\n",
        "\n",
        "#인코더\n",
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # num_layers만큼 쌓아올린 인코더의 층.\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "sflAg2rEE4bJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "OtyWum6ZivYA"
      },
      "outputs": [],
      "source": [
        "#디코더 레이어\n",
        "\n",
        "# 디코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "\n",
        "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
        "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "\n",
        "#디코더\n",
        "\n",
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "  # 패딩 마스크\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "j7s0QBhFFAMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "9FhV7Ol8f_EE"
      },
      "outputs": [],
      "source": [
        "def transformer(\n",
        "    vocab_size,\n",
        "    num_layers,\n",
        "    units,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dropout,\n",
        "    name=\"transformer\"\n",
        "   ):\n",
        "    ## encoder input & decoder input\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    ## 인코더에서 패딩을 위한 마스크\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None),\n",
        "        name='enc_padding_mask'\n",
        "        )(inputs)\n",
        "\n",
        "    ## 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "    ## 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "        create_look_ahead_mask,\n",
        "        output_shape=(1, None, None),\n",
        "        name='look_ahead_mask'\n",
        "        )(dec_inputs)\n",
        "\n",
        "    ## 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "    ## 디코더에서 패딩을 위한 마스크\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    ## 인코더\n",
        "    enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "    )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    ## 디코더\n",
        "    dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    ## 완전연결층\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ev-KXb1gE39"
      },
      "source": [
        "### 6. 모델 생성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter"
      ],
      "metadata": {
        "id": "-uysdPeTFqDc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "nBXv_K09gDiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b250f68-c2a9-4a2f-f7db-836dc32a751d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " inputs (InputLayer)         [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)     [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
            "                                                                                                  \n",
            " encoder (Functional)        (None, None, 512)            2309427   ['inputs[0][0]',              \n",
            "                                                          2          'enc_padding_mask[0][0]']    \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)    (None, 1, None, None)        0         ['dec_inputs[0][0]']          \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
            "                                                                                                  \n",
            " decoder (Functional)        (None, None, 512)            2940416   ['dec_inputs[0][0]',          \n",
            "                                                          0          'encoder[0][0]',             \n",
            "                                                                     'look_ahead_mask[0][0]',     \n",
            "                                                                     'dec_padding_mask[0][0]']    \n",
            "                                                                                                  \n",
            " outputs (Dense)             (None, None, 8164)           4188132   ['decoder[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 56686564 (216.24 MB)\n",
            "Trainable params: 56686564 (216.24 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## 하이퍼파라미터\n",
        "NUM_LAYERS = 6 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수\n",
        "UNITS = 2048 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.15 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "_6yPIZ_KFstK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = tf.multiply(loss, mask) ## padding 부분은 빼고 loss를 계산한다?\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "Npa1hGIyZmkS"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate"
      ],
      "metadata": {
        "id": "RhxEW174Fxzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "e-dYvGGQF2gD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Compile"
      ],
      "metadata": {
        "id": "Kelq-5s_Fvjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "Wzp9YtyxF6x6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ets30dmNgGr9"
      },
      "source": [
        "### 7. Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "R6UEAMKsgHuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90089a7-34a5-48f8-8f5b-de5aa72848d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "185/185 [==============================] - 200s 921ms/step - loss: 2.7264 - accuracy: 0.0531\n",
            "Epoch 2/30\n",
            "185/185 [==============================] - 167s 904ms/step - loss: 2.1726 - accuracy: 0.1017\n",
            "Epoch 3/30\n",
            "185/185 [==============================] - 169s 912ms/step - loss: 1.9957 - accuracy: 0.1043\n",
            "Epoch 4/30\n",
            "185/185 [==============================] - 168s 907ms/step - loss: 1.9101 - accuracy: 0.1089\n",
            "Epoch 5/30\n",
            "185/185 [==============================] - 168s 908ms/step - loss: 1.8378 - accuracy: 0.1132\n",
            "Epoch 6/30\n",
            "185/185 [==============================] - 167s 902ms/step - loss: 1.7754 - accuracy: 0.1161\n",
            "Epoch 7/30\n",
            "185/185 [==============================] - 167s 904ms/step - loss: 1.7126 - accuracy: 0.1191\n",
            "Epoch 8/30\n",
            "185/185 [==============================] - 167s 902ms/step - loss: 1.6522 - accuracy: 0.1223\n",
            "Epoch 9/30\n",
            "185/185 [==============================] - 167s 904ms/step - loss: 1.5915 - accuracy: 0.1250\n",
            "Epoch 10/30\n",
            "185/185 [==============================] - 167s 902ms/step - loss: 1.5301 - accuracy: 0.1288\n",
            "Epoch 11/30\n",
            "185/185 [==============================] - 167s 900ms/step - loss: 1.4644 - accuracy: 0.1326\n",
            "Epoch 12/30\n",
            "185/185 [==============================] - 167s 901ms/step - loss: 1.4011 - accuracy: 0.1369\n",
            "Epoch 13/30\n",
            "185/185 [==============================] - 167s 900ms/step - loss: 1.3333 - accuracy: 0.1424\n",
            "Epoch 14/30\n",
            "185/185 [==============================] - 167s 901ms/step - loss: 1.2637 - accuracy: 0.1492\n",
            "Epoch 15/30\n",
            "185/185 [==============================] - 166s 899ms/step - loss: 1.1926 - accuracy: 0.1563\n",
            "Epoch 16/30\n",
            "185/185 [==============================] - 167s 904ms/step - loss: 1.1250 - accuracy: 0.1641\n",
            "Epoch 17/30\n",
            "185/185 [==============================] - 166s 899ms/step - loss: 1.0583 - accuracy: 0.1726\n",
            "Epoch 18/30\n",
            "185/185 [==============================] - 167s 901ms/step - loss: 0.9976 - accuracy: 0.1807\n",
            "Epoch 19/30\n",
            "185/185 [==============================] - 166s 896ms/step - loss: 0.9375 - accuracy: 0.1892\n",
            "Epoch 20/30\n",
            "185/185 [==============================] - 166s 895ms/step - loss: 0.8916 - accuracy: 0.1957\n",
            "Epoch 21/30\n",
            "185/185 [==============================] - 165s 894ms/step - loss: 0.8520 - accuracy: 0.2008\n",
            "Epoch 22/30\n",
            "185/185 [==============================] - 167s 901ms/step - loss: 0.8203 - accuracy: 0.2055\n",
            "Epoch 23/30\n",
            "185/185 [==============================] - 166s 895ms/step - loss: 0.7810 - accuracy: 0.2114\n",
            "Epoch 24/30\n",
            "185/185 [==============================] - 166s 898ms/step - loss: 0.7362 - accuracy: 0.2197\n",
            "Epoch 25/30\n",
            "185/185 [==============================] - 166s 896ms/step - loss: 0.7038 - accuracy: 0.2256\n",
            "Epoch 26/30\n",
            "185/185 [==============================] - 166s 896ms/step - loss: 0.6761 - accuracy: 0.2300\n",
            "Epoch 27/30\n",
            "185/185 [==============================] - 166s 895ms/step - loss: 0.6459 - accuracy: 0.2364\n",
            "Epoch 28/30\n",
            "185/185 [==============================] - 167s 901ms/step - loss: 0.6255 - accuracy: 0.2403\n",
            "Epoch 29/30\n",
            "185/185 [==============================] - 166s 898ms/step - loss: 0.6035 - accuracy: 0.2449\n",
            "Epoch 30/30\n",
            "185/185 [==============================] - 167s 900ms/step - loss: 0.5853 - accuracy: 0.2485\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    epochs=30,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## loss 시각화\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "DpKDhGyT--FP",
        "outputId": "abb85991-596d-4982-d93e-838f6dd0613d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7957b8566380>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Model Evaluation"
      ],
      "metadata": {
        "id": "G8lUsppHjZQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_inference(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    ## 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "    ## ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "    sentence = tf.expand_dims(\n",
        "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "    ## 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "    ## 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    ## 디코더의 인퍼런스 단계\n",
        "    for i in range(MAX_LENGTH):\n",
        "        ## 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "        predictions = predictions[:, -1:, :]\n",
        "\n",
        "        ## 현재 예측한 단어의 정수\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        ## 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        ## 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "        ## 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output_sequence, axis=0)"
      ],
      "metadata": {
        "id": "VDnggIOwiSrS"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Try Conversation"
      ],
      "metadata": {
        "id": "mOmuBWrAjjJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(sentence):\n",
        "    ## 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "    prediction = decoder_inference(sentence)\n",
        "\n",
        "    ## 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "    predicted_sentence = tokenizer.decode(\n",
        "        [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "    print('입력 : {}'.format(sentence))\n",
        "    print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "    return predicted_sentence"
      ],
      "metadata": {
        "id": "ZSUlxhsXiT9y"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('컴퓨터가 망가졌어')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dopa5yxdiUTy",
        "outputId": "50015f0c-e930-4e51-c733-35c650a8791d"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 컴퓨터가 망가졌어\n",
            "출력 : 잘 찾아보세요 .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'잘 찾아보세요 .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('알고리즘이 아까는 잘 됐는데 갑자기 안돼')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "VpYof950iZSo",
        "outputId": "209a129a-0388-4789-947c-25ad5eea3432"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 알고리즘이 아까는 잘 됐는데 갑자기 안돼\n",
            "출력 : 다른 사람 말은 한 귀로 흘리세요 .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'다른 사람 말은 한 귀로 흘리세요 .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('사랑해')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B6X18Z95kZcx",
        "outputId": "c2546c2b-f622-4a36-9539-e401c9a55076"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 사랑해\n",
            "출력 : 그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('혹시 여기 무간지옥이야?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "IumZDPVUilQx",
        "outputId": "ccaa617d-82b2-4df5-f913-6f0cde673b24"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 혹시 여기 무간지옥이야?\n",
            "출력 : 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 변화도 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('뭐야? 엄청 무섭다 ㄷㄷ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lCxIqQPujt9B",
        "outputId": "96e794f0-ee7c-45c1-cfc6-820d75b7fe59"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 뭐야? 엄청 무섭다 ㄷㄷ\n",
            "출력 : 그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('그러는 이유가 뭐야')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bwZ2oesMj9DC",
        "outputId": "4e875730-33ea-4228-dc05-fc707b161f8a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 그러는 이유가 뭐야\n",
            "출력 : 그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니그러니'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('제주도 빨리 가고 싶다.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "5M_bwvTakCAa",
        "outputId": "70662e53-fe47-4a76-8c56-c21de5deab93"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 제주도 빨리 가고 싶다.\n",
            "출력 : \u0014졌다졌다졌다졌다졌다졌다졌다그러니그러니그러니그러니졌다졌다졌다졌다졌다졌다졌다그러니\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\x14졌다졌다졌다졌다졌다졌다졌다그러니그러니그러니그러니졌다졌다졌다졌다졌다졌다졌다그러니'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('물건이 망가졌어')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "EUG11np1kOMS",
        "outputId": "ce0b6a8f-ad5b-45eb-fb27-f65fbf19ceb2"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 물건이 망가졌어\n",
            "출력 : 로부터 로부터 로부터 로부터 로부터 부족한가로부터 부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'로부터 로부터 로부터 로부터 로부터 부족한가로부터 부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가부족한가'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 회고\n",
        "\n",
        "왜 잘 되던것도 런타임이 끊기고 나서 다시 돌리면 안되는 걸까? 실행하는 순서에 따라 문제가 생길 수 있어서 순서에 따라서 코드를 작성하고 있는데... 아까 잘 되었던 것을 다시 바꿀수도 없고... 다시 바꿔도 새로운 문제가 생겨나고...\n",
        "문제 가 생겼을때 해결하는 게 참 힘들다\n",
        "\n",
        "\n",
        "\n",
        "그리고 처음 2번째는 문장의 형태로 생성이 되는 것 같은데 그 이후부터느 생성이 이상하게 되는것 같다."
      ],
      "metadata": {
        "id": "7Jpc3WWVbfnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YADrAkYfcMGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}